# -*- coding: utf-8 -*-
"""ppi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8xRqB1bvfLoTvICrMtHFuvKqdvU5x6D
"""

# !pip install dgl-cu101

import dgl
from dgl.data.ppi import PPIDataset

from dgl.data.ppi import LegacyPPIDataset
from torch.utils.data import DataLoader
import torch
import numpy as np


def collate(sample):
    graphs, feats, labels = map(list, zip(*sample))
    graph = dgl.batch(graphs)
    feats = torch.from_numpy(np.concatenate(feats))
    labels = torch.from_numpy(np.concatenate(labels))
    return graph, feats, labels

train_dataset = LegacyPPIDataset(mode="train")
# train_dataloader = DataLoader(train_dataset, batch_size=1, collate_fn=collate)

# graphs = list(train_dataloader)
ids = np.random.permutation(len(train_dataset))

G1 = train_dataset.train_graphs[ids[0]]
G2 = train_dataset.train_graphs[ids[1]]

A1 = np.asarray(G1.adjacency_matrix_scipy().todense())
A2 = np.asarray(G2.adjacency_matrix_scipy().todense())

if len(A1) < len(A2):
    A1, A2 = A2, A1
print(A1.shape, A2.shape)

L1 = train_dataset.train_labels[ids[0]]
L2 = train_dataset.train_labels[ids[1]]

A1[A1 > 0] = 1
A2[A2 > 0] = 1

import torch
import torch.nn.functional as F
import torch.nn as nn

# n_nodes = 100
A1 = torch.FloatTensor(A1).cuda()
A2 = torch.FloatTensor(A2).cuda()

#
from sklearn.metrics import f1_score

# def compute_f1(pA, A):
#     pA = torch.sigmoid(pA).detach().cpu().numpy()
#     pA[pA >= 0.5] = 1 
#     pA[pA < 0.5] = 0
#     A = A.cpu().numpy()
#     f1 = f1_score(A, pA, average="micro")
#     return f1
def compute_f1(pA, A):
    pA = pA.detach().cpu().numpy()
    pA[pA >= 0.5] = 1 
    pA[pA < 0.5] = 0
    A = A.cpu().numpy()
    f1 = f1_score(A, pA, average="micro")
    return f1

# class Model(nn.Module):
#     def __init__(self):
#         super().__init__()
#         l = A1.shape[0]
#         l2 = A2.shape[0]
#         self.X1 = nn.Embedding(l, ori_dim)
#         self.X2 = nn.Embedding(l, ori_dim)
#         self.linear1 = nn.Linear(l*(l-1)//2, ori_dim, bias=False)

#         self.linear2 = nn.Linear(l2*(l2-1)//2, l2*(l2-1)//2, bias=False)

        
#     def forward(self):
#         x1 = torch.pdist(self.X1.weight, p=2)
#         x1 = self.linear1(x1)
#         # x1 = x1.to('cuda:1')
#         x1 = self.linear2(x1)
#         x1 = torch.sigmoid(x1)

#         x2 = torch.pdist(self.X2.weight, p=2)
#         x2 = self.linear1(x2)
#         # x2 = x2.to('cuda:1')
#         x2 = self.linear2(x2)
#         x2 = torch.sigmoid(x2)
#         return x1, x2



# class Model(nn.Module):
#     def __init__(self):
#         super().__init__()
#         l = A1.shape[0]
#         l2 = A2.shape[0]
#         ori_dim = 1024
#         self.D = nn.Embedding(len(A1), ori_dim)
#         # self.D2 = nn.Embedding(l2, ori_dim)
#         nn.init.uniform_(self.D.weight, 0., 1.)
#         # nn.init.uniform_(self.D2.weight, 0., 1.)
#         self.M = nn.Linear(l, l2, bias=False)
  
#     # 4 x 4 (4x5) ->T -> 5 x 4 
#     def forward(self):
#         x = self.D.weight
#         pa = x.mm(x.t())
#         # x2 = self.D2.weight
#         x2 = self.M(x.t()).t()
#         pa2 = x2.mm(x2.t())
#         return pa, pa2

# model = Model().cuda()
# loss_fn = nn.MSELoss()
# optim = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)
# for iter in range(1000):
#     model.train()
#     optim.zero_grad()
#     pred_A1, pred_A2 = model()
#     loss = loss_fn(pred_A1, A1) + loss_fn(pred_A2, A2)
#     if iter%10 == 0:
#         microf11 = compute_f1(pred_A1, A1)
#         microf12 = compute_f1(pred_A2, A2)
#         print(f"Iter {iter} - loss {loss:.4f} - f1 {microf11:.3f}  {microf12:.3f}")
#     loss.backward()
#     optim.step()


# SYMMETRIC SVD
from scipy.sparse.linalg import svds
import scipy.sparse as sparse
def symmetric_svd(adj, dim_size):
    adj = adj.cpu().numpy()
    adj = sparse.csr_matrix(adj)
    U, X,_ = svds(adj, k = dim_size)
    embedding = U*(X**1)
    embedding = torch.FloatTensor(embedding).cuda()
    return embedding
X1 = symmetric_svd(A1, 1024)
# X1 = torch.sigmoid(X1)

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        l = A1.shape[0]
        l2 = A2.shape[0]
        self.M = nn.Linear(l, l2, bias=False)
  
    # 4 x 4 (4x5) ->T -> 5 x 4 
    def forward(self):
        x2 = self.M(X1.t()).t()
        pa2 = x2.mm(x2.t())
        return pa2
model = Model().cuda()
loss_fn = nn.MSELoss()
optim = torch.optim.Adam(model.parameters(), lr=0.1, weight_decay=5e-4)
for iter in range(1000):
    model.train()
    optim.zero_grad()
    pred_A2 = model()
    loss = loss_fn(pred_A2, A2)
    if iter%10 == 0:
        pred_A1 = X1.mm(X1.t())
        microf11 = compute_f1(pred_A1, A1)
        microf12 = compute_f1(pred_A2, A2)
        print(f"Iter {iter} - loss {loss:.4f} - f1 {microf11:.3f}  {microf12:.3f}")
    loss.backward()
    optim.step()
# END SYMMETRIC SVD

# gen edgelist, labels, featuresh
# X1 = model.D.weight
X2 = model.M(X1.t()).t()
features = X1.detach().cpu().numpy()
edgelist = np.argwhere(A1.detach().cpu().numpy() > 0)
labels = L1

import os
outdir = "data-autoencoder/ppi/0"
if not os.path.isdir(outdir):
    os.makedirs(outdir)

with open(outdir + "/edgelist.txt", "w+") as fp:
    for src, trg in edgelist:
        fp.write(f"{src} {trg}\n")
with open(outdir + "/labels.txt", "w+") as fp:
    for i, label in enumerate(labels):
        label = np.argwhere(label > 0).flatten()
        fp.write("{} {}\n".format(i, " ".join([str(x) for x in label])))
np.savez_compressed(outdir + "/features.npz", features=features)

features = X2.detach().cpu().numpy()
edgelist = np.argwhere(A2.detach().cpu().numpy() > 0)
labels = L2

outdir = "data-autoencoder/ppi/1"
if not os.path.isdir(outdir):
    os.makedirs(outdir)

with open(outdir + "/edgelist.txt", "w+") as fp:
    for src, trg in edgelist:
        fp.write(f"{src} {trg}\n")
with open(outdir + "/labels.txt", "w+") as fp:
    for i, label in enumerate(labels):
        label = np.argwhere(label > 0).flatten()
        fp.write("{} {}\n".format(i, " ".join([str(x) for x in label])))
np.savez_compressed(outdir + "/features.npz", features=features)


"""

for i in 0 1
do
echo $i 
python -u main.py --dataset temp/data-autoencoder/ppi/$i/ \
    --init ori --cuda graphsage --aggregator mean > logs/ppi$i.log
done

python -u main.py --dataset temp/data-autoencoder/ppi/1/ --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-0-ori-40.pkl > logs/ppi1-transfer-from-0.log
python -u main.py --dataset temp/data-autoencoder/ppi/0/ --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-1-ori-40.pkl > logs/ppi0-transfer-from-1.log
"""

# python -u main.py --dataset temp/data-autoencoder/ppi/1/ --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-0-ori-40.pkl > logs/ppi1-transfer-from-0.log
# python -u main.py --dataset temp/data-autoencoder/ppi/0/ --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-1-ori-40.pkl > logs/ppi0-transfer-from-1.log