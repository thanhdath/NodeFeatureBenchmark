# -*- coding: utf-8 -*-
"""ppi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i8xRqB1bvfLoTvICrMtHFuvKqdvU5x6D
"""

# !pip install dgl-cu101

import dgl
from dgl.data.ppi import PPIDataset

from dgl.data.ppi import LegacyPPIDataset
from torch.utils.data import DataLoader
import torch
import numpy as np
import torch
import torch.nn.functional as F
import torch.nn as nn


def collate(sample):
    graphs, feats, labels = map(list, zip(*sample))
    graph = dgl.batch(graphs)
    feats = torch.from_numpy(np.concatenate(feats))
    labels = torch.from_numpy(np.concatenate(labels))
    return graph, feats, labels

train_dataset = LegacyPPIDataset(mode="train")
# train_dataloader = DataLoader(train_dataset, batch_size=1, collate_fn=collate)

# graphs = list(train_dataloader)
ids = np.random.permutation(len(train_dataset))
ids = [i for i in ids if train_dataset.train_graphs[i].number_of_nodes() < 1500]

G1 = train_dataset.train_graphs[ids[0]]
G2 = train_dataset.train_graphs[ids[1]]

A1 = np.asarray(G1.adjacency_matrix_scipy().todense())
A2 = np.asarray(G2.adjacency_matrix_scipy().todense())
A1[A1 > 0] = 1
A2[A2 > 0] = 1

# if len(A1) < len(A2):
#     A1, A2 = A2, A1
#     G1, G2 = G2, G1
print(A1.shape, A2.shape)

L1 = train_dataset.train_labels[ids[0]]
L2 = train_dataset.train_labels[ids[1]]
pL1 = np.abs(L1.sum(axis=0)/len(L1) - 0.5)
pL2 = np.abs(L2.sum(axis=0)/len(L2) - 0.5)
print("Label percentage:")
print(pL1)
print(pL2)
scores = pL1*pL2
selected_label = np.argmin(scores)
print("Selected label: ", selected_label)
L1 = L1[:, selected_label]
L2 = L2[:, selected_label]

F1 = train_dataset.features[train_dataset.train_mask_list[ids[0]]]
F2 = train_dataset.features[train_dataset.train_mask_list[ids[1]]]

# n_nodes = 100
A1 = torch.FloatTensor(A1).cuda()
A2 = torch.FloatTensor(A2).cuda()

#
from sklearn.metrics import f1_score
def compute_f1(pA, A):
    pA = pA.detach().cpu().numpy()
    pA[pA >= 0.5] = 1 
    pA[pA < 0.5] = 0
    A = A.cpu().numpy()
    f1 = f1_score(A, pA, average="micro")
    return f1

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.D1 = torch.FloatTensor(F1).cuda()
        self.D2 = torch.FloatTensor(F2).cuda()
        fdim = self.D1.shape[1]
        self.W = nn.Sequential(
            nn.Linear(fdim, fdim*2, bias=True),
            nn.ReLU(),
            nn.Linear(fdim*2, fdim*2, bias=True),
            nn.ReLU(),
            nn.Linear(fdim*2, fdim, bias=True),
        )
        
    def forward(self):
        D1 = self.W(self.D1)
        x1 = D1.mm(D1.t())
        D2 = self.W(self.D2)
        x2 = D2.mm(D2.t())
        return x1, x2

model = Model().cuda()
loss_fn = nn.MSELoss()
optim = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
for iter in range(400):
    model.train()
    optim.zero_grad()
    pred_A1, pred_A2 = model()
    loss = loss_fn(pred_A1, A1) + loss_fn(pred_A2, A2)
    loss.backward()
    optim.step()
    if iter%50 == 0:
        microf11 = compute_f1(pred_A1, A1)
        microf12 = compute_f1(pred_A2, A2)
        print(f"Iter {iter} - loss {loss:.4f} - f1 {microf11:.3f}  {microf12:.3f}")
    

# gen edgelist, labels, featuresh
print("Save graphs")
# gen edgelist, labels, featuresh
# X1 = model.X1.weight.detach().cpu().numpy()
# X2 = model.M(X1.t()).t().detach().cpu().numpy()
X1 = F1
X2 = F2
features = X1
edgelist = np.argwhere(A1.detach().cpu().numpy() > 0)
labels = L1

import os
outdir = "data-autoencoder/ppi/0"
if not os.path.isdir(outdir):
    os.makedirs(outdir)

with open(outdir + "/edgelist.txt", "w+") as fp:
    for src, trg in edgelist:
        fp.write(f"{src} {trg}\n")
with open(outdir + "/labels.txt", "w+") as fp:
    for i, label in enumerate(labels):
        fp.write(f"{i} {label}\n")
np.savez_compressed(outdir + "/features.npz", features=features)

features = X2
edgelist = np.argwhere(A2.detach().cpu().numpy() > 0)
labels = L2

outdir = "data-autoencoder/ppi/1"
if not os.path.isdir(outdir):
    os.makedirs(outdir)

with open(outdir + "/edgelist.txt", "w+") as fp:
    for src, trg in edgelist:
        fp.write(f"{src} {trg}\n")
with open(outdir + "/labels.txt", "w+") as fp:
    for i, label in enumerate(labels):
        fp.write(f"{i} {label}\n")
np.savez_compressed(outdir + "/features.npz", features=features)

# python -u main.py --dataset temp/data-autoencoder/ppi/1/ --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-0-ori-40.pkl > logs/ppi1-transfer-from-0.log
# python -u main.py --dataset temp/data-autoencoder/ppi/0/ --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-1-ori-40.pkl > logs/ppi0-transfer-from-1.log


"""
for i in 0 1
do
echo $i
    python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/$i --init ori --cuda gat > logs/ppi$i.log
done
python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/1 --init ori --cuda gat --load-model gat-best-model-0-ori-40.pkl > logs/ppi1-tf-0.log
python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/0 --init ori --cuda gat --load-model gat-best-model-1-ori-40.pkl > logs/ppi0-tf-1.log

"""

"""
for i in 0 1
do
echo $i
    python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/$i --init ori --cuda graphsage --aggregator mean > logs/ppi$i.log
done
python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/1 --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-0-ori-40.pkl > logs/ppi1-tf-0.log
python -u -W ignore main.py --dataset temp/data-autoencoder/ppi/0 --init ori --cuda graphsage --aggregator mean --load-model graphsage-best-model-1-ori-40.pkl > logs/ppi0-tf-1.log

"""